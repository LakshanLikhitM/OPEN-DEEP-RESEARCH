{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdW627ypvAD1"
      },
      "source": [
        "# Research Topic Assistant\n",
        "\n",
        "This notebook takes a research topic, generates sub-questions using an LLM, retrieves answers, and summarizes the results into a final paragraph.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dc91a17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b15d936-32cb-40ee-a64a-789802c47429"
      },
      "source": [
        "!pip install openai"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.8.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZFjEF1jgvAD6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "import json\n",
        "from typing import List, Dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LeHfbtujvAD6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "import json\n",
        "from typing import List, Dict\n",
        "from google.colab import userdata\n",
        "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhgQ_lOevAD6"
      },
      "source": [
        "## Step 1: Generate Sub-Questions from Research Topic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5_t9YI32vAD7"
      },
      "outputs": [],
      "source": [
        "def generate_sub_questions(topic: str, num_questions: int = 3) -> List[str]:\n",
        "    \"\"\"\n",
        "    Uses an LLM to generate sub-questions related to the research topic.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"Given the research topic: \"{topic}\"\n",
        "\n",
        "Generate exactly {num_questions} specific sub-questions that would help someone understand this topic better.\n",
        "The questions should be diverse and cover different aspects of the topic.\n",
        "\n",
        "Return ONLY the questions, one per line, numbered 1-{num_questions}.\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful research assistant that generates insightful questions.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        max_tokens=300\n",
        "    )\n",
        "\n",
        "    # Parse the response to extract questions\n",
        "    questions_text = response.choices[0].message.content.strip()\n",
        "    questions = []\n",
        "\n",
        "    for line in questions_text.split('\\n'):\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            question = line.split('. ', 1)[-1] if '. ' in line else line\n",
        "            question = question.split(') ', 1)[-1] if ') ' in line else question\n",
        "            questions.append(question)\n",
        "\n",
        "    return questions[:num_questions]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWX44LggvAD7"
      },
      "source": [
        "## Step 2: Retrieve Answers for Sub-Questions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VlDiXeY5vAD8"
      },
      "outputs": [],
      "source": [
        "def get_answer(question: str, topic: str) -> str:\n",
        "    \"\"\"\n",
        "    Uses an LLM to generate a short response to a sub-question.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"Context: We are researching \"{topic}\"\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Provide a concise answer in 2-3 sentences.\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a knowledgeable research assistant. Provide clear, concise answers.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        max_tokens=150\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "\n",
        "def retrieve_answers(questions: List[str], topic: str) -> List[Dict[str, str]]:\n",
        "    \"\"\"\n",
        "    Retrieves answers for all sub-questions.\n",
        "    \"\"\"\n",
        "    qa_pairs = []\n",
        "\n",
        "    for question in questions:\n",
        "        answer = get_answer(question, topic)\n",
        "        qa_pairs.append({\n",
        "            'question': question,\n",
        "            'answer': answer\n",
        "        })\n",
        "\n",
        "    return qa_pairs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbHyf3W0vAD8"
      },
      "source": [
        "## Step 3: Summarize Results into Final Paragraph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "atI0H4bEvAD8"
      },
      "outputs": [],
      "source": [
        "def summarize_results(topic: str, qa_pairs: List[Dict[str, str]]) -> str:\n",
        "    \"\"\"\n",
        "    Synthesizes all Q&A pairs into a coherent final paragraph.\n",
        "    \"\"\"\n",
        "    # Format the Q&A pairs for the prompt\n",
        "    qa_text = \"\\n\\n\".join([\n",
        "        f\"Q: {pair['question']}\\nA: {pair['answer']}\"\n",
        "        for pair in qa_pairs\n",
        "    ])\n",
        "\n",
        "    prompt = f\"\"\"Research Topic: \"{topic}\"\n",
        "\n",
        "Here are answers to key questions about this topic:\n",
        "\n",
        "{qa_text}\n",
        "\n",
        "Please write a single comprehensive paragraph (4-6 sentences) that synthesizes all the information above into a coherent overview of the topic. The paragraph should flow naturally and provide a complete understanding of the research topic.\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert at synthesizing information into clear, coherent summaries.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        max_tokens=300\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ni1d5CBrvAD9"
      },
      "source": [
        "## Research Assistant Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KTOhuhUfvAD9"
      },
      "outputs": [],
      "source": [
        "def research_assistant(topic: str) -> Dict:\n",
        "    \"\"\"\n",
        "    Complete research assistant pipeline.\n",
        "\n",
        "    Args:\n",
        "        topic: The research topic to explore\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing questions, answers, and final summary\n",
        "    \"\"\"\n",
        "    print(f\"Researching topic: {topic}\\n\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    print(\"\\nStep 1: Generating sub-questions...\\n\")\n",
        "    questions = generate_sub_questions(topic, num_questions=3)\n",
        "\n",
        "    for i, q in enumerate(questions, 1):\n",
        "        print(f\"  {i}. {q}\")\n",
        "\n",
        "    print(\"\\nStep 2: Retrieving answers...\\n\")\n",
        "    qa_pairs = retrieve_answers(questions, topic)\n",
        "\n",
        "    for i, pair in enumerate(qa_pairs, 1):\n",
        "        print(f\"  Q{i}: {pair['question']}\")\n",
        "        print(f\"  A{i}: {pair['answer']}\\n\")\n",
        "\n",
        "    print(\"Step 3: Generating final summary...\\n\")\n",
        "    summary = summarize_results(topic, qa_pairs)\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"\\nFINAL SUMMARY:\\n\")\n",
        "    print(summary)\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "\n",
        "    return {\n",
        "        'topic': topic,\n",
        "        'questions': questions,\n",
        "        'qa_pairs': qa_pairs,\n",
        "        'summary': summary\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2a_zOaIvAD9"
      },
      "source": [
        "## Run the Research Assistant\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zwVB99fvAD9",
        "outputId": "15fbc1cf-80ca-4e3c-a8da-bfde3dbd7706"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Researching topic: AI vs Human\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Step 1: Generating sub-questions...\n",
            "\n",
            "  1. What are the specific areas where AI has surpassed human performance and what implications does this have for the future?\n",
            "  2. How does the process of human decision-making differ from that of AI, especially in terms of ethics and unpredictability?\n",
            "  3. How does the proliferation of AI in various sectors affect human job security and what are the potential solutions to this problem?\n",
            "\n",
            "Step 2: Retrieving answers...\n",
            "\n",
            "  Q1: What are the specific areas where AI has surpassed human performance and what implications does this have for the future?\n",
            "  A1: AI has surpassed human performance in areas such as data analysis, image recognition, and certain games like chess and Go, given its ability to process large amounts of data quickly and accurately. This suggests a future where tasks requiring detailed pattern recognition and data processing could be automated, potentially leading to increased efficiency but also job displacement in certain sectors.\n",
            "\n",
            "  Q2: How does the process of human decision-making differ from that of AI, especially in terms of ethics and unpredictability?\n",
            "  A2: Human decision-making is influenced by emotions, personal experiences, and ethical and moral considerations, which can lead to unpredictable outcomes. On the other hand, AI decision-making is based on algorithms and data patterns, lacking emotional input and ethical judgement, and typically follows predictable logic.\n",
            "\n",
            "  Q3: How does the proliferation of AI in various sectors affect human job security and what are the potential solutions to this problem?\n",
            "  A3: The proliferation of AI in various sectors can threaten human job security as it has the potential to automate many tasks traditionally performed by humans, leading to job displacement. Potential solutions include re-skilling and up-skilling the workforce to equip them for jobs that require human traits like creativity and emotional intelligence, and implementing policies for fair wealth distribution and social security measures to cope with possible unemployment.\n",
            "\n",
            "Step 3: Generating final summary...\n",
            "\n",
            "================================================================================\n",
            "\n",
            "FINAL SUMMARY:\n",
            "\n",
            "AI has demonstrated superiority over humans in tasks such as data analysis, image recognition, and strategic games like chess and Go, owing to its capacity for rapid and precise data processing. This advancement foretells a future where roles necessitating intricate pattern recognition and data analysis could be automated, potentially enhancing efficiency but also posing a threat of job displacement in certain industries. In contrast, human decision-making is shaped by emotions, personal experiences, and ethical considerations, resulting in unpredictable outcomes, while AI decisions are algorithm-driven and predictable, lacking emotional and ethical nuances. The increasing integration of AI across sectors raises concerns about human job security due to the potential for automation to replace traditional human roles, prompting the need for workforce re-skilling, up-skilling, and policy initiatives focused on wealth distribution and social security to manage potential unemployment challenges.\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# INPUT: Enter your research topic here\n",
        "research_topic = \"AI vs Human\"\n",
        "\n",
        "results = research_assistant(research_topic)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}